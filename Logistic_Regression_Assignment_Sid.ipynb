{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K9x_0EnNUEt"
      },
      "source": [
        "#Logistic Regression Theoretical Questions\n",
        "\n",
        "####1. **What is Logistic Regression, and how does it differ from Linear Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "Logistic Regression is a classification algorithm used to predict categorical outcomes, typically binary. Unlike Linear Regression, which predicts continuous values, Logistic Regression predicts the probability of a class using the sigmoid function to map predictions between 0 and 1.\n",
        "\n",
        "---\n",
        "\n",
        "####2. **What is the mathematical equation of Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "The mathematical equation of Logistic Regression is:\n",
        "\n",
        "$$\n",
        "P(y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + \\exp^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n",
        "$$\n",
        "\n",
        "This gives the probability of the outcome being class 1. The probability of class 0 is:\n",
        "\n",
        "$$\n",
        "P(y = 0 \\mid \\mathbf{x}) = 1 - P(y = 1 \\mid \\mathbf{x})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "####3. **Why do we use the Sigmoid function in Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "The sigmoid function maps any real-valued number to a value between 0 and 1, making it ideal for predicting probabilities in binary classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "####4. **What is the cost function of Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "The cost function is:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "####5. **What is Regularization in Logistic Regression? Why is it needed ?**\n",
        "\n",
        "**Answer:**\n",
        "Regularization prevents overfitting by penalizing large coefficient values. It adds a penalty term (L1, L2, or both) to the cost function, encouraging simpler models.\n",
        "\n",
        "---\n",
        "\n",
        "####6. **Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Lasso (L1)**: Adds absolute value of coefficients; promotes sparsity.\n",
        "* **Ridge (L2)**: Adds squared values of coefficients; discourages large coefficients.\n",
        "* **Elastic Net**: Combines L1 and L2; balances sparsity and shrinkage.\n",
        "\n",
        "---\n",
        "\n",
        "####7. **When should we use Elastic Net instead of Lasso or Ridge ?**\n",
        "\n",
        "**Answer:**\n",
        "Elastic Net is preferred when there are multiple correlated features, as it combines the strengths of both Lasso and Ridge.\n",
        "\n",
        "---\n",
        "\n",
        "####8. **What is the impact of the regularization parameter (λ) in Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "A higher λ increases regularization, reducing model complexity and possibly underfitting. A lower λ decreases regularization, increasing model complexity and risk of overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "####9. **What are the key assumptions of Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* No multicollinearity among independent variables.\n",
        "* Linear relationship between log-odds and independent variables.\n",
        "* Large sample size for reliable results.\n",
        "* Independent observations.\n",
        "\n",
        "---\n",
        "\n",
        "####10. **What are some alternatives to Logistic Regression for classification tasks ?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* Decision Trees\n",
        "* Random Forest\n",
        "* Support Vector Machines (SVM)\n",
        "* K-Nearest Neighbors (KNN)\n",
        "* Naive Bayes\n",
        "* Neural Networks\n",
        "\n",
        "---\n",
        "\n",
        "####11. **What are Classification Evaluation Metrics ?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-Score\n",
        "* ROC-AUC\n",
        "* Confusion Matrix\n",
        "\n",
        "---\n",
        "\n",
        "####12. **How does class imbalance affect Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "Class imbalance can bias the model toward the majority class, leading to poor performance on the minority class. Solutions include using class weights, oversampling, or undersampling.\n",
        "\n",
        "---\n",
        "\n",
        "####13. **What is Hyperparameter Tuning in Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "It involves finding the best values for hyperparameters like regularization strength (C) and penalty type using techniques like GridSearchCV or RandomizedSearchCV to improve performance.\n",
        "\n",
        "---\n",
        "\n",
        "####14. **What are different solvers in Logistic Regression? Which one should be used ?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **liblinear**: Good for small datasets and L1 regularization.\n",
        "* **lbfgs**: Efficient for large datasets.\n",
        "* **saga**: Supports all types of penalties and is suitable for large datasets.\n",
        "* **newton-cg**: Good for L2 penalty.\n",
        "\n",
        "Use the solver depending on dataset size and regularization type.\n",
        "\n",
        "---\n",
        "\n",
        "####15. **How is Logistic Regression extended for multiclass classification ?**\n",
        "\n",
        "**Answer:**\n",
        "It uses:\n",
        "\n",
        "* **One-vs-Rest (OvR)**: Trains one classifier per class.\n",
        "* **Multinomial (Softmax)**: Generalizes logistic regression to multiple classes directly.\n",
        "\n",
        "---\n",
        "\n",
        "####16. **What are the advantages and disadvantages of Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "**Advantages**:\n",
        "\n",
        "* Simple and interpretable\n",
        "* Works well for binary classification\n",
        "* Efficient and requires fewer resources\n",
        "\n",
        "**Disadvantages**:\n",
        "\n",
        "* Assumes linearity in log-odds\n",
        "* Struggles with complex relationships\n",
        "* Sensitive to outliers\n",
        "\n",
        "---\n",
        "\n",
        "####17. **What are some use cases of Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* Email spam detection\n",
        "* Credit risk assessment\n",
        "* Disease diagnosis (e.g., diabetes prediction)\n",
        "* Customer churn prediction\n",
        "* Marketing campaign response prediction\n",
        "\n",
        "---\n",
        "\n",
        "####18. **What is the difference between Softmax Regression and Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Logistic Regression**: Used for binary classification.\n",
        "* **Softmax Regression**: Used for multiclass classification; generalizes logistic regression to more than two classes.\n",
        "\n",
        "---\n",
        "\n",
        "####19. **How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification ?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **OvR**: Simpler, works well with smaller datasets.\n",
        "* **Softmax**: Preferred for large datasets and when mutual exclusivity between classes is assumed.\n",
        "\n",
        "---\n",
        "\n",
        "####20. **How do we interpret coefficients in Logistic Regression ?**\n",
        "\n",
        "**Answer:**\n",
        "Coefficients represent the change in the log-odds of the outcome for a one-unit change in the predictor. A positive coefficient increases the odds of the target class, while a negative one decreases it.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKbyd5bBWiaM",
        "outputId": "36a7bc5a-8f34-479b-8b03-36ae04f05ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 0.9561\n"
          ]
        }
      ],
      "source": [
        "##Practical Questions\n",
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3gy-ZubZQiL",
        "outputId": "fbe990ab-f027-48e2-fefd-954533935048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with L1 regularization: 0.9561\n"
          ]
        }
      ],
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Logistic Regression with L1 penalty (Lasso)\n",
        "# Note: 'liblinear' or 'saga' solver supports L1 penalty\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with L1 regularization: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syTd-VRnZQZ0",
        "outputId": "ad5a1229-ab76-40df-a1ae-b271d7d67426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9561\n",
            "Model coefficients:\n",
            "mean radius: 1.0274\n",
            "mean texture: 0.2215\n",
            "mean perimeter: -0.3621\n",
            "mean area: 0.0255\n",
            "mean smoothness: -0.1562\n",
            "mean compactness: -0.2377\n",
            "mean concavity: -0.5326\n",
            "mean concave points: -0.2837\n",
            "mean symmetry: -0.2267\n",
            "mean fractal dimension: -0.0365\n",
            "radius error: -0.0971\n",
            "texture error: 1.3706\n",
            "perimeter error: -0.1814\n",
            "area error: -0.0872\n",
            "smoothness error: -0.0225\n",
            "compactness error: 0.0474\n",
            "concavity error: -0.0429\n",
            "concave points error: -0.0324\n",
            "symmetry error: -0.0347\n",
            "fractal dimension error: 0.0116\n",
            "worst radius: 0.1117\n",
            "worst texture: -0.5089\n",
            "worst perimeter: -0.0156\n",
            "worst area: -0.0169\n",
            "worst smoothness: -0.3077\n",
            "worst compactness: -0.7727\n",
            "worst concavity: -1.4286\n",
            "worst concave points: -0.5109\n",
            "worst symmetry: -0.7469\n",
            "worst fractal dimension: -0.1009\n"
          ]
        }
      ],
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Logistic Regression with L2 penalty (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model coefficients:\")\n",
        "for feature, coef in zip(data.feature_names, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bThwEoLAZQRQ",
        "outputId": "6a8ca563-f1dd-478e-ba80-907d76273726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with Elastic Net regularization: 0.9737\n"
          ]
        }
      ],
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Logistic Regression with elasticnet penalty\n",
        "# Note: 'saga' solver is required for elasticnet penalty\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    l1_ratio=0.5,          # Balance between L1 and L2 (0=l2, 1=l1)\n",
        "    solver='saga',\n",
        "    max_iter=5000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with Elastic Net regularization: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W_HpUswZQJb",
        "outputId": "8af93320-6d37-4590-aa8e-5d162c0af21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set accuracy (One-vs-Rest): 0.9667\n"
          ]
        }
      ],
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Load multiclass dataset (Iris)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Wrap LogisticRegression with OneVsRestClassifier for OvR strategy\n",
        "ovr_model = OneVsRestClassifier(\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "ovr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = ovr_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy (One-vs-Rest): {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNio5VsCZQBH",
        "outputId": "3363ac56-0c28-4ebb-c660-627cea1ce3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Test set accuracy with best parameters: 0.9825\n"
          ]
        }
      ],
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']  # 'liblinear' supports both l1 and l2\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict and evaluate on test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with best parameters: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgLjL9RuZP5T",
        "outputId": "6270ec3a-097e-4e8b-8f1b-b4140de63453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy (Stratified K-Fold CV): 0.9543\n"
          ]
        }
      ],
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Initialize Stratified K-Fold (e.g., 5 folds)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold CV\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Initialize and train Logistic Regression\n",
        "    model = LogisticRegression(max_iter=10000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Print average accuracy across folds\n",
        "print(f\"Average accuracy (Stratified K-Fold CV): {np.mean(accuracies):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-we_fwnZPxO",
        "outputId": "fc6c1b7a-8442-45f0-8d4e-3005013ee447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7989\n"
          ]
        }
      ],
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# URL of online CSV file (example: Titanic dataset from GitHub)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "\n",
        "# Load dataset from URL\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Example preprocessing for Titanic dataset:\n",
        "# We'll predict 'Survived' and use a few numeric/categorical columns\n",
        "# Fill missing 'Age' with median, fill missing 'Embarked' with mode\n",
        "data['Age'] = data['Age'].fillna(data['Age'].median())\n",
        "data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
        "\n",
        "# Select features and target\n",
        "X = data[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
        "y = data['Survived']\n",
        "\n",
        "# Convert categorical columns using one-hot encoding\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNvZD1CXZPpJ",
        "outputId": "2a020346-1776-4ece-e557-dd1a0164c822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found: {'logreg__C': np.float64(3.347086111390218), 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
            "Test set accuracy with best parameters: 0.9737\n"
          ]
        }
      ],
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create pipeline: scaling + logistic regression\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=5000, random_state=42))\n",
        "])\n",
        "\n",
        "# Valid solver-penalty combinations with corrected 'None' (Python None, not string)\n",
        "param_dist = [\n",
        "    {\n",
        "        'logreg__solver': ['liblinear'],\n",
        "        'logreg__penalty': ['l1', 'l2'],\n",
        "        'logreg__C': uniform(0.01, 10)\n",
        "    },\n",
        "    {\n",
        "        'logreg__solver': ['lbfgs', 'newton-cg', 'sag'],\n",
        "        'logreg__penalty': ['l2'],  # these solvers support only l2 penalty\n",
        "        'logreg__C': uniform(0.01, 10)\n",
        "    },\n",
        "    {\n",
        "        'logreg__solver': ['saga'],\n",
        "        'logreg__penalty': ['l1', 'l2', 'elasticnet', None],  # Use None without quotes here\n",
        "        'logreg__C': uniform(0.01, 10),\n",
        "        'logreg__l1_ratio': uniform(0, 1)  # l1_ratio used only if penalty='elasticnet', ignored otherwise\n",
        "    }\n",
        "]\n",
        "\n",
        "best_score = -np.inf\n",
        "best_params = None\n",
        "best_estimator = None\n",
        "\n",
        "for params in param_dist:\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=pipe,\n",
        "        param_distributions=params,\n",
        "        n_iter=10,\n",
        "        scoring='accuracy',\n",
        "        cv=5,\n",
        "        verbose=1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    if random_search.best_score_ > best_score:\n",
        "        best_score = random_search.best_score_\n",
        "        best_params = random_search.best_params_\n",
        "        best_estimator = random_search.best_estimator_\n",
        "\n",
        "print(\"Best parameters found:\", best_params)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_estimator.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with best parameters: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lluZoZfkZPgP",
        "outputId": "e3fab1be-dcca-4ce2-e7a6-31a43179d508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 0.9833\n"
          ]
        }
      ],
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load multiclass dataset\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create One-vs-One classifier with Logistic Regression\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=1000, solver='liblinear'))\n",
        "\n",
        "# Train the model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "rPqM6jjRZPXc",
        "outputId": "8c1ceb97-a363-4973-8770-0c3f04a00dfc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot confusion matrix\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od7AfXG4ZPPG",
        "outputId": "93e73d54-a015-447b-8f3c-dc9e4de7e6fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.946\n",
            "Recall: 0.986\n",
            "F1-Score: 0.966\n"
          ]
        }
      ],
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load example dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall: {recall:.3f}\")\n",
        "print(f\"F1-Score: {f1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU4xAN2vZPGg",
        "outputId": "d6fcd8d3-5a8d-49a2-f52b-121276bfdaaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without class weights:\n",
            "[[174   6]\n",
            " [ 10  10]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       180\n",
            "           1       0.62      0.50      0.56        20\n",
            "\n",
            "    accuracy                           0.92       200\n",
            "   macro avg       0.79      0.73      0.76       200\n",
            "weighted avg       0.91      0.92      0.92       200\n",
            "\n",
            "With class weights:\n",
            "[[160  20]\n",
            " [  2  18]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.94       180\n",
            "           1       0.47      0.90      0.62        20\n",
            "\n",
            "    accuracy                           0.89       200\n",
            "   macro avg       0.73      0.89      0.78       200\n",
            "weighted avg       0.94      0.89      0.90       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Generate imbalanced binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1 (imbalanced)\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train logistic regression without class weights\n",
        "clf_no_weights = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = clf_no_weights.predict(X_test)\n",
        "\n",
        "print(\"Without class weights:\")\n",
        "print(confusion_matrix(y_test, y_pred_no_weights))\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Train logistic regression with balanced class weights\n",
        "clf_weighted = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "clf_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = clf_weighted.predict(X_test)\n",
        "\n",
        "print(\"With class weights:\")\n",
        "print(confusion_matrix(y_test, y_pred_weighted))\n",
        "print(classification_report(y_test, y_pred_weighted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rp9SpaTZO_N",
        "outputId": "bcadd0f6-28a8-4c69-d5f5-133de95db063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8100558659217877\n",
            "Precision: 0.7857142857142857\n",
            "Recall: 0.7432432432432432\n",
            "F1 Score: 0.7638888888888888\n"
          ]
        }
      ],
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Target and features\n",
        "target = 'survived'\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "\n",
        "X = titanic[features]\n",
        "y = titanic[target]\n",
        "\n",
        "# Define numeric and categorical features\n",
        "numeric_features = ['age', 'sibsp', 'parch', 'fare', 'pclass']\n",
        "categorical_features = ['sex', 'embarked']\n",
        "\n",
        "# Numeric transformer: impute missing with median\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Categorical transformer: impute missing with most frequent and one-hot encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessors\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create pipeline with preprocessor and logistic regression model\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV0RU_XoZO3Y",
        "outputId": "f8c5150c-3dbf-4eb5-cec2-86d3f022c34a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy without scaling: 0.956140350877193\n",
            "Accuracy with scaling: 0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model without feature scaling\n",
        "raw_model = LogisticRegression(max_iter=10000)\n",
        "raw_model.fit(X_train, y_train)\n",
        "raw_accuracy = raw_model.score(X_test, y_test)\n",
        "\n",
        "# Apply standardization (feature scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train model on scaled data\n",
        "scaled_model = LogisticRegression(max_iter=1000)\n",
        "scaled_model.fit(X_train_scaled, y_train)\n",
        "scaled_accuracy = scaled_model.score(X_test_scaled, y_test)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy without scaling:\", raw_accuracy)\n",
        "print(\"Accuracy with scaling:\", scaled_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-csmjMZkZOvU",
        "outputId": "e7bbd06c-f489-4a55-8226-775a3e89890e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC-AUC Score: 0.9977\n"
          ]
        }
      ],
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Evaluate ROC-AUC\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGG9hBtWZOnA",
        "outputId": "90dcbc89-e50a-413f-f126-21a8b47a0b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with C=0.5: 0.9649122807017544\n"
          ]
        }
      ],
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with custom regularization strength (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "print(\"Accuracy with C=0.5:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fG8mW0eZOfM",
        "outputId": "fb65645c-5693-467b-f573-b10891850fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 important features based on Logistic Regression coefficients:\n",
            "                 Feature  Coefficient  Absolute Coefficient\n",
            "26       worst concavity    -1.428595              1.428595\n",
            "11         texture error     1.370567              1.370567\n",
            "0            mean radius     1.027437              1.027437\n",
            "25     worst compactness    -0.772709              0.772709\n",
            "28        worst symmetry    -0.746894              0.746894\n",
            "6         mean concavity    -0.532558              0.532558\n",
            "27  worst concave points    -0.510929              0.510929\n",
            "21         worst texture    -0.508877              0.508877\n",
            "2         mean perimeter    -0.362135              0.362135\n",
            "24      worst smoothness    -0.307731              0.307731\n"
          ]
        }
      ],
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance from coefficients\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Absolute Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Display top features\n",
        "print(\"Top 10 important features based on Logistic Regression coefficients:\")\n",
        "print(feature_importance.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz0DA3ctZOXY",
        "outputId": "a4a3a8c0-58fe-47e1-9e75-b03089aef3ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cohen’s Kappa Score: 0.9053\n"
          ]
        }
      ],
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen’s Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen’s Kappa Score: {kappa_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "3gqIlav3ZOPl",
        "outputId": "120d4645-903e-4d57-8f74-af921cb2aab0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "PrecisionRecallDisplay.from_estimator(model, X_test, y_test)\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATb52sr_ZOIi",
        "outputId": "aa2c5558-c0dd-40b6-9ac1-3811b63ccf5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solver: liblinear, Accuracy: 0.9561\n",
            "Solver: saga, Accuracy: 0.9737\n",
            "Solver: lbfgs, Accuracy: 0.9561\n"
          ]
        }
      ],
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Evaluate each solver\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=10000)\n",
        "        model.fit(X_train, y_train)\n",
        "        accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "        print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Solver: {solver} failed with error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCvz5gisZOAe",
        "outputId": "e9f3a0b4-cc07-4bb5-8035-a945f5a06b50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9068\n"
          ]
        }
      ],
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute MCC\n",
        "y_pred = model.predict(X_test)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CBVXIb7ZN44",
        "outputId": "b8f8c0c6-f087-4ea2-e4a8-f7afa1d1ce8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on raw data: 0.9561\n",
            "Accuracy on standardized data: 0.9737\n"
          ]
        }
      ],
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "raw_model = LogisticRegression(max_iter=10000)\n",
        "raw_model.fit(X_train, y_train)\n",
        "raw_accuracy = accuracy_score(y_test, raw_model.predict(X_test))\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "scaled_model = LogisticRegression(max_iter=10000)\n",
        "scaled_model.fit(X_train_scaled, y_train)\n",
        "scaled_accuracy = accuracy_score(y_test, scaled_model.predict(X_test_scaled))\n",
        "\n",
        "# Compare results\n",
        "print(f\"Accuracy on raw data: {raw_accuracy:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {scaled_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FlZBQrHZNwT",
        "outputId": "229e02b2-c93d-45ee-bc93-0da79bfaf896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best C: 0.5\n",
            "Accuracy with best C: 0.776536312849162\n"
          ]
        }
      ],
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Preprocess\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']].fillna(0)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Grid search for optimal C\n",
        "param_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Output\n",
        "print(\"Best C:\", grid.best_params_['C'])\n",
        "print(\"Accuracy with best C:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OySdixw4ZNn_",
        "outputId": "7d2329ca-57ce-4994-969a-acb3f5be5589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloaded model accuracy: 0.9561\n"
          ]
        }
      ],
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import joblib\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model to file\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "\n",
        "# Load the model from file\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Reloaded model accuracy: {accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
